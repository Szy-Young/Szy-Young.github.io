<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziyang Song</title>
  
  <meta name="author" content="Ziyang Song">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziyang Song</name>
              </p>
              <p>
                I am a final year PhD student at <a href="https://www.polyu.edu.hk">The Hong Kong Polytechnic University</a>, supervised by <a href="https://yang7879.github.io">Prof. Bo Yang</a>.
                Currently I am also interning at <a href="https://lifeattiktok.com/">TikTok</a> (San Jose, CA) with <a href="https://scholar.google.com.hk/citations?user=A8e8UNAAAAAJ&hl=zh-CN">Xinyu Gong</a>.
              </p>
              <p>
                Previously, I got my M.Eng and B.Eng degrees (<a href="https://en.wikipedia.org/wiki/Special_Class_for_the_Gifted_Young">Honors Youth Program</a>) from <a href="http://www.xjtu.edu.cn">Xi'an Jiaotong University</a>.
                During my M.Eng study, I interned at <a href="https://www.sensetime.com/en">SenseTime</a> and <a href="https://www.tencent.com/en-us/business/robotics.html">Tencent Robotics X</a>.
              </p>
              <p>
                My general research interests lie in computer vision and machine learning. Currently I am focusing on reconstruction and generation of dynamic 3D scenes.
              </p>
              <p>
                <span style="color: red; font-weight: bold;">Actively seeking full-time industry opportunities starting Spring 2026!</span>
              </p>
              <p style="text-align:center">
                <a href="mailto:ziyang.song@connect.polyu.hk">Email</a> &nbsp/&nbsp
                <a href="images/CV_Ziyang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/ziyang-song-87760a142">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=7YcpCEwAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Szy-Young">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ziyang_daily.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ziyang_daily.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='24_icml_osn'>
                <img width=100% src="images/24_icml_osn.gif">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.05615"><papertitle>OSN: Infinite Representations of Dynamic 3D Scenes from Monocular Videos</papertitle></a>
              <br>
              <strong>Ziyang Song</strong>, Jinxi Li, Bo Yang
              <br>
              <em>International Conference on Machine Learning (<strong>ICML</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2407.05615">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=LQjRLq39UFg"><font color="red">Video</font></a> /
              <a href="https://github.com/vLAR-group/OSN"><font color="red">Code</font></a>
              <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=OSN&type=star&count=true&size=small"
              frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              <p></p>
              <p>
                 The first framework to represent dynamic 3D scenes in infinitely many ways from a monocular RGB video.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='24_tpami_ogc'>
                <img width=100% src="images/22_neurips_ogc.gif">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10551495"><papertitle>Unsupervised 3D Object Segmentation of Point Clouds by Geometry Consistency</papertitle></a>
              <br>
              <strong>Ziyang Song</strong>, Bo Yang
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2024
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10551495">IEEE Xplore</a> /
              <a href="https://www.youtube.com/watch?v=dZBjvKWJ4K0"><font color="red">Video</font></a> /
              <a href="https://github.com/vLAR-group/OGC"><font color="red">Code</font></a>
              <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=OGC&type=star&count=true&size=small"
              frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              <p></p>
              <p>
                The journal version of our OGC (NeurIPS 2022). More experiments and analysis are included.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='23_neurips_nvfi'>
                <img width=100% src="images/23_neurips_nvfi.gif">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.06398"><papertitle>NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos</papertitle></a>
              <br>
              Jinxi Li, <strong>Ziyang Song</strong>, Bo Yang
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2312.06398">arXiv</a> /
              <a href="https://github.com/vLAR-group/NVFi"><font color="red">Code</font></a>
              <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=NVFi&type=star&count=true&size=small"
              frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              <p></p>
              <p>
                A novel representation of dynamic 3D scenes by disentangling physical velocities from geometry and appearance, enabling: 1) future frame extrapolation, 2) unsupervised semantic scene decomposition, and 3) velocity transfer.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='23_iccv_actformer'>
                <img width=100% src="images/23_iccv_actformer.gif">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://liangxuy.github.io/actformer/"><papertitle>ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation</papertitle></a>
              <br>
              Liang Xu*, <strong>Ziyang Song*</strong>, Dongliang Wang, Jing Su, Zhicheng Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, Wenjun Zeng, Wei Wu
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2203.07706">arXiv</a> /
              <a href="https://liangxuy.github.io/actformer/"><font color="red">Project Page</font></a> /
              <a href="https://github.com/Szy-Young/ActFormer"><font color="red">Code</font></a>
              <iframe src="https://ghbtns.com/github-btn.html?user=Szy-Young&repo=ActFormer&type=star&count=true&size=small"
              frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              <p></p>
              (* denotes equal contribution)
              <p>
                A GAN-based Transformer for general action-conditioned 3D human motion generation, including single-person actions and multi-person interactive actions.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id='22_neurips_ogc'>
                <img width=100% src="images/22_neurips_ogc.gif">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.04458"><papertitle>OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds</papertitle></a>
              <br>
              <strong>Ziyang Song</strong>, Bo Yang
              <br>
              <em>Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2210.04458">arXiv</a> /
              <a href="https://www.youtube.com/watch?v=dZBjvKWJ4K0"><font color="red">Video</font></a> /
              <a href="https://github.com/vLAR-group/OGC"><font color="red">Code</font></a>
              <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=OGC&type=star&count=true&size=small"
              frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
              <p></p>
              <p>
                We propose the first unsupervised 3D object segmentation method, learning from dynamic motion patterns in point cloud sequences.
              </p>
            </td>
          </tr>

<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one" id='20_accv_peaug'>-->
<!--                <img width=100% src="images/20_accv_peaug.jpg">-->
<!--              </div>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/2007.08071"><papertitle>Learning End-to-End Action Interaction by Paired-Embedding Data Augmentation</papertitle></a>-->
<!--              <br>-->
<!--              <strong>Ziyang Song</strong>, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling, Shenghao Zhang-->
<!--              <br>-->
<!--              <em>Asian Conference on Computer Vision (<strong>ACCV</strong>)</em>, 2020-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2007.08071">arXiv</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--                We specify a new task to learn end-to-end action interaction from unlabeled interactive action pairs, and a Paired-Embedding (PE) data augmentation method for efficient learning with small data.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>					-->
          
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one" id='20_icpr_hri'>-->
<!--                <img width=100% src="images/20_icpr_hri.gif">-->
<!--              </div>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/2007.01065"><papertitle>Attention-Oriented Action Recognition for Real-Time Human-Robot Interaction</papertitle></a>-->
<!--              <br>-->
<!--              <strong>Z. Song</strong>, Ziyi Yin, Zejian Yuan, Chong Zhang, Wanchao Chi, Yonggen Ling, Shenghao Zhang-->
<!--              <br>-->
<!--              <em>International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2020-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/2007.01065">arXiv</a>-->
<!--              <p></p>-->
<!--              <p>-->
<!--                A framework for real-time human action recognition from RGB-D videos in human-robot interaction (HRI) scenarios.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>		-->

          </tbody>
        </table>


<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>Teaching Assistants</heading>-->
<!--                <p> <strong>Fall Term, 2023</strong>: ‚ÄÇ‚ÄÇ <em>Computer Graphics</em> (The Hong Kong Polytechnic University) </p>-->
<!--                <p> <strong>Spring Term, 2023</strong>: ‚ÄÇ‚ÄÇ <em>Creative Digital Media Design</em> (The Hong Kong Polytechnic University) </p>-->
<!--                <p> <strong>Fall Term, 2022</strong>: ‚ÄÇ‚ÄÇ <em>Machine Learning and Data Analytics</em> (The Hong Kong Polytechnic University) </p>-->
<!--                <p> <strong>Spring Term, 2022</strong>: ‚ÄÇ‚ÄÇ  <em>Operating Systems</em> (The Hong Kong Polytechnic University) </p>-->
<!--                <p> <strong>Fall Term, 2021</strong>: ‚ÄÇ‚ÄÇ <em>Data Structures and Database Systems</em> (The Hong Kong Polytechnic University). </p>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->

        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
          <tbody><tr>
            <td><br>
                <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
            <p align="right"><font size="2"> Last update: 2023.12. <a href="https://jonbarron.info/">Thanks.</a></font></p>
            </td>
          </tr>
          </tbody>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>
